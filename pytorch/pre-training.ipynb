{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "\n",
    "from mae import MaskedAutoEncoder  # Import your MAE model (adjust path if necessary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset for grayscale images (1 channel)\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # Get list of image file paths (adjust extension if needed)\n",
    "        self.image_paths = glob(os.path.join(root_dir, \"*.png\"))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        # Read the image using torchvision.io.read_image and normalize to [0, 1]\n",
    "        image = read_image(img_path).float() / 255.0  # Image in 1 channel (grayscale)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the image transformations (resize to 224x224)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), antialias=True)\n",
    "])\n",
    "\n",
    "# Define paths for data directories\n",
    "data_dir = \"/home/alcindo/projeto/masked-autoencoders-cows/data\"\n",
    "train_dir = os.path.join(data_dir, \"train\")\n",
    "val_dir = os.path.join(data_dir, \"validation\")\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = ImageDataset(root_dir=train_dir, transform=transform)\n",
    "val_dataset = ImageDataset(root_dir=val_dir, transform=transform)\n",
    "\n",
    "# Create DataLoaders with multi-processing and pinned memory for efficiency\n",
    "train_loader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size= BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the MAE model with in_channels=1 for grayscale images and send it to device\n",
    "model = MaskedAutoEncoder(\n",
    "    emb_size=1024, \n",
    "    decoder_emb_size=512, \n",
    "    patch_size=16, \n",
    "    num_head=8,\n",
    "    encoder_num_layers= 6, \n",
    "    decoder_num_layers=4, \n",
    "    in_channels=1, \n",
    "    img_size=224\n",
    ").to(device)\n",
    "\n",
    "# Define the optimizer (AdamW)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Train Loss: 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Validation Loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 19\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     train_loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     22\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Training loop with progress bar\n",
    "    train_loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Train\", leave=False)\n",
    "    for images in train_loop:\n",
    "        images = images.to(device)  # Move images to GPU\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss, _, _ = model(images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Validation loop with progress bar\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_loop = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Val\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images in val_loop:\n",
    "            images = images.to(device)\n",
    "            loss, _, _ = model(images)\n",
    "            val_loss += loss.item()\n",
    "            val_loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint after each epoch\n",
    "    # torch.save(model.state_dict(), f\"mae_checkpoint_epoch{epoch+1}.pth\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mae-cows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
